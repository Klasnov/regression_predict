{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/cs5228_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## 1. Load in the dataset and remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Remove the features that:\n",
    "      1. may not be useful for the model\n",
    "      2. have too many missing values\n",
    "      3. all the values are the same\n",
    "      4. may be redundant with other features\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.drop([\"listing_id\", \"indicative_price\", \"eco_category\", \"fuel_type\", \"opc_scheme\", \"lifespan\", \"features\", \"accessories\"], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = load_data(\"data/train.csv\")\n",
    "df_test = load_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proce_date(df):\n",
    "    \"\"\"\n",
    "    Convert the date imformation from string to duration numerical number\n",
    "    \"\"\"\n",
    "    # Convert date columns to datetime\n",
    "    df[\"reg_date\"] = pd.to_datetime(df[\"reg_date\"], format=\"%d-%b-%Y\")\n",
    "    df[\"original_reg_date\"] = pd.to_datetime(df[\"original_reg_date\"], format=\"%d-%b-%Y\")\n",
    "\n",
    "    # Calculate age of vehicle\n",
    "    end_date = pd.to_datetime(\"2024-11-01\")\n",
    "    reg_age = round(((end_date - df[\"reg_date\"]).dt.days / 365.25), 2)\n",
    "\n",
    "    # Fill missing values in manufactured column\n",
    "    df.loc[df[\"category\"].str.contains(\"parf car\"), \"manufactured\"] = df.loc[\n",
    "        df[\"category\"].str.contains(\"parf car\"), \"manufactured\"\n",
    "    ].fillna(df[\"reg_date\"].dt.year)\n",
    "\n",
    "    # Fill missing values in manufactured column\n",
    "    df.loc[df[\"manufactured\"].isna(), \"manufactured\"] = (\n",
    "        df.loc[df[\"manufactured\"].isna(), \"manufactured\"]\n",
    "        .fillna(df[\"original_reg_date\"].dt.year)\n",
    "        .fillna(df[\"reg_date\"].dt.year)\n",
    "    )\n",
    "    df[\"manufactured\"] = 2024 - df[\"manufactured\"]\n",
    "\n",
    "    # Drop and rename columns\n",
    "    df[\"reg_date\"] = reg_age\n",
    "    df.rename(columns={\"reg_date\": \"reg_age\"}, inplace=True)\n",
    "    df.drop([\"original_reg_date\"], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = proce_date(df_train)\n",
    "df_test = proce_date(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encode the basic information\n",
    "\n",
    "### 3.1 Encoded `type_of_Vehicle` and `transmission` features using numerical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = list(df_train[\"type_of_vehicle\"].unique())\n",
    "tran = list(df_train[\"transmission\"].unique())\n",
    "\n",
    "type_map = {t: i for i, t in enumerate(type)}\n",
    "tran_map = {t: i for i, t in enumerate(tran)}\n",
    "\n",
    "def encode_type_trans(df):\n",
    "    \"\"\"\n",
    "    Encode the type of vehicle and transmission into numerical values\n",
    "    \"\"\"\n",
    "    df[\"type_of_vehicle\"] = df[\"type_of_vehicle\"].map(type_map)\n",
    "    df[\"transmission\"] = df[\"transmission\"].map(tran_map)\n",
    "    return df\n",
    "\n",
    "df_train = encode_type_trans(df_train)\n",
    "df_test = encode_type_trans(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encode free text features utilizing BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def encode_title(df, target_dimension=16):\n",
    "    \"\"\"\n",
    "    Encode the title feature using BERT\n",
    "    \"\"\"\n",
    "    # Clean the title feature by removing anything in parentheses\n",
    "    df[[\"title_cleaned\", \"coe_valid_to\"]] = df[\"title\"].str.extract(r\"^(.*)\\s\\(\\D+\\s(.*)\\)$\")\n",
    "    df[\"title_cleaned\"] = df[\"title_cleaned\"].fillna(df[\"title\"])\n",
    "    df[\"title\"] = df[\"title_cleaned\"]\n",
    "    df.drop([\"title_cleaned\", \"coe_valid_to\"], axis=1, inplace=True)\n",
    "    # Add the prefix \"CAR: \"\n",
    "    df.loc[:, \"title\"] = \"CAR: \" + df[\"title\"]\n",
    "\n",
    "    # Encode the title using BERT\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = \"bert-large-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def encode_text(text):\n",
    "        # Tokenize the text and get the embeddings\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True,\n",
    "                           truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        embeddings = embeddings.mean(dim=1)\n",
    "        return embeddings.squeeze().cpu().numpy() \n",
    "    \n",
    "    # Encode the title using BERT\n",
    "    title_embeddings = df[\"title\"].apply(encode_text).tolist()\n",
    "    embedding_matrix = np.vstack(title_embeddings)\n",
    "    pca = PCA(n_components=target_dimension)\n",
    "    reduced_embeddings = pca.fit_transform(embedding_matrix)\n",
    "    df[\"title\"] = reduced_embeddings.tolist()\n",
    "\n",
    "    # Drop the redundant information\n",
    "    df.drop([\"make\", \"model\"], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_description(df, target_dimension=8):\n",
    "    \"\"\"\n",
    "    Encode the description feature using BERT\n",
    "    \"\"\"\n",
    "    # Fill the missing values in the description\n",
    "    df[\"description\"] = df[\"description\"].fillna(\"Good condition\")\n",
    "\n",
    "    # Encode the description using BERT\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = \"bert-large-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def encode_text(text):\n",
    "        # Tokenize the text and get the embeddings\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True,\n",
    "                           truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        embeddings = embeddings.mean(dim=1)\n",
    "        return embeddings.squeeze().cpu().numpy() \n",
    "    \n",
    "    # Encode the description using BERT\n",
    "    description_embeddings = df[\"description\"].apply(encode_text).tolist()\n",
    "    embedding_matrix = np.vstack(description_embeddings)\n",
    "    pca = PCA(n_components=target_dimension)\n",
    "    reduced_embeddings = pca.fit_transform(embedding_matrix)\n",
    "    df[\"description\"] = reduced_embeddings.tolist()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_free_text(df):\n",
    "    # Encode the title and description\n",
    "    df = encode_title(df)\n",
    "    df = encode_description(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = encode_free_text(df_train)\n",
    "df_test = encode_free_text(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Save the basic features encoded dataset to new CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"data/train_basic_encoded.csv\", index=False)\n",
    "df_test.to_csv(\"data/test_basic_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"data/train_basic_encoded.csv\")\n",
    "df_test = pd.read_csv(\"data/test_basic_encoded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encode the `category` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def multi_label_category(df):\n",
    "    \"\"\"\n",
    "    Convert the category feature into multi-label format\n",
    "    \"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_category = mlb.fit_transform(df[\"category\"].str.split(\", \"))\n",
    "    df[\"category\"] = df_category.tolist()\n",
    "    return df\n",
    "\n",
    "df_train = multi_label_category(df_train)\n",
    "df_test = multi_label_category(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fill the missing numerical values utilizing clustering imformation\n",
    "\n",
    "### 5.1 Fill the missing values in the features related to the vehicle's condition\n",
    "\n",
    "### 5.2 Fill the missing values in the features related to the vehicle's mileage or age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            25000 non-null  object \n",
      " 1   description      25000 non-null  object \n",
      " 2   manufactured     25000 non-null  float64\n",
      " 3   reg_age          25000 non-null  float64\n",
      " 4   type_of_vehicle  25000 non-null  int64  \n",
      " 5   category         25000 non-null  object \n",
      " 6   transmission     25000 non-null  int64  \n",
      " 7   curb_weight      24693 non-null  float64\n",
      " 8   power            22360 non-null  float64\n",
      " 9   engine_cap       24404 non-null  float64\n",
      " 10  no_of_owners     24982 non-null  float64\n",
      " 11  depreciation     24493 non-null  float64\n",
      " 12  coe              25000 non-null  int64  \n",
      " 13  road_tax         22368 non-null  float64\n",
      " 14  dereg_value      24780 non-null  float64\n",
      " 15  mileage          19696 non-null  float64\n",
      " 16  omv              24936 non-null  float64\n",
      " 17  arf              24826 non-null  float64\n",
      " 18  price            25000 non-null  float64\n",
      "dtypes: float64(13), int64(3), object(3)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title              [2.260293483734131, -0.15082983672618866, -0.5...\n",
      "description        [1.51604425907135, -0.05233699455857277, -0.09...\n",
      "manufactured                                                     7.0\n",
      "reg_age                                                         6.84\n",
      "type_of_vehicle                                                    1\n",
      "category            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
      "transmission                                                       0\n",
      "curb_weight                                                   1465.0\n",
      "power                                                          135.0\n",
      "engine_cap                                                    1991.0\n",
      "no_of_owners                                                     2.0\n",
      "depreciation                                                 21170.0\n",
      "coe                                                            47002\n",
      "road_tax                                                      1202.0\n",
      "dereg_value                                                  45179.0\n",
      "mileage                                                      85680.0\n",
      "omv                                                          40678.0\n",
      "arf                                                          43950.0\n",
      "price                                                        96800.0\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.loc[:, [\"title\", \"type_of_vehicle\", \"category\", \"transmission\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## 1. Normalize the features to avoid the influence of different dataset distributions\n",
    "\n",
    "## 2. Train the model (XGBoost) with validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
